{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ckkok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(use_saved_file: bool = False):\n",
    "    if (use_saved_file): \n",
    "        df = pd.read_pickle(\"./review_df.pkl\") \n",
    "        return df\n",
    "\n",
    "    # Reading the data\n",
    "    code_df = pd.read_csv(\"../../data/anime_codes.csv\")\n",
    "\n",
    "    # Read the review data\n",
    "    reviews_dir = \"../../data/reviews\"\n",
    "    all_reviews = list()\n",
    "\n",
    "    for index, review_doc in enumerate(os.listdir(reviews_dir)):\n",
    "        current_dict = dict()\n",
    "            \n",
    "        # Get the current anime's code\n",
    "        current_dict['code'] = review_doc.split('.')[0]\n",
    "        \n",
    "        # Get the current anime's reviews\n",
    "        f = open(os.path.join(reviews_dir, review_doc), 'r', encoding=\"utf-8\")\n",
    "        current_dict['review'] = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        all_reviews.append(current_dict)\n",
    "\n",
    "    # Create a dataframe of the anime codes and their respective reviews\n",
    "    review_df = pd.DataFrame(all_reviews)\n",
    "\n",
    "    # Match the name and rating of animes in code_df to the anime reviews dataframe\n",
    "    review_df['code']=review_df['code'].astype(int)\n",
    "    df = pd.merge(review_df, code_df, on='code')\n",
    "\n",
    "\n",
    "    #Utitlity functions for removing ASCII characters, converting lower case, removing stop words, html and punctuation from description\n",
    "    def _removeNonAscii(s):\n",
    "        return \"\".join(i for i in s if  ord(i)<128)\n",
    "\n",
    "    def make_lower_case(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_stop_words(text):\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "\n",
    "    def remove_html(text):\n",
    "        html_pattern = re.compile('<.*?>')\n",
    "        return html_pattern.sub(r'', text)\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text = tokenizer.tokenize(text)\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "\n",
    "    df['cleaned'] = df['review'].apply(_removeNonAscii)\n",
    "    df['cleaned'] = df.cleaned.apply(func = make_lower_case)\n",
    "    df['cleaned'] = df.cleaned.apply(func = remove_stop_words)\n",
    "    df['cleaned'] = df.cleaned.apply(func=remove_punctuation)\n",
    "    df['cleaned'] = df.cleaned.apply(func=remove_html)\n",
    "    df.name = df.name.apply(lambda x: re.sub(r\"\\s\\s*\", \" \", re.sub(r\"[\\-\\_]\", \" \", x)) )\n",
    "    with open(\"./review_df.pkl\", \"wb\") as filehandle:\n",
    "        pickle.dump(df, filehandle)\n",
    "        filehandle.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_data(use_saved_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting the description into words\n",
    "corpus = []\n",
    "for words in df['cleaned']:\n",
    "    corpus.append(words.split())\n",
    "\n",
    "# Using the Google pretrained Word2Vec Model \n",
    "# If using for the first time, download and store in ../../data/ \n",
    "# (link: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz)\n",
    "\n",
    "EMBEDDING_FILE = '../../data/GoogleNews-vectors-negative300.bin.gz'\n",
    "google_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "# Training our corpus with Google Pretrained Model\n",
    "\n",
    "google_model = Word2Vec(size = 300, window=5, min_count = 2, workers = -1)\n",
    "google_model.build_vocab(corpus)\n",
    "\n",
    "#model.intersect_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', lockf=1.0, binary=True)\n",
    "\n",
    "google_model.intersect_word2vec_format(EMBEDDING_FILE, lockf=1.0, binary=True)\n",
    "\n",
    "google_model.train(corpus, total_examples=google_model.corpus_count, epochs = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ckkok\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\ckkok\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# Generate the average word2vec for the each set of anime reviews\n",
    "def vectors(x: pd.DataFrame):\n",
    "\n",
    "    # Creating a list for storing the vectors (description into vectors)\n",
    "    global array_embeddings\n",
    "    array_embeddings = []\n",
    "\n",
    "    # Reading the each anime review set\n",
    "    for line in df['cleaned']:\n",
    "        avgword2vec = None\n",
    "        count = 0\n",
    "        for word in line.split():\n",
    "            if word in google_model.wv.vocab:\n",
    "                count += 1\n",
    "                if avgword2vec is None:\n",
    "                    avgword2vec = google_model[word]\n",
    "                else:\n",
    "                    avgword2vec = avgword2vec + google_model[word]\n",
    "\n",
    "        if avgword2vec is not None:\n",
    "            avgword2vec = avgword2vec / count\n",
    "\n",
    "            array_embeddings.append(avgword2vec)\n",
    "\n",
    "# Calling the function vectors\n",
    "vectors(df)\n",
    "\n",
    "# finding cosine similarity for the vectors\n",
    "cosine_similarities = cosine_similarity(array_embeddings, array_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(title: str, df: pd.DataFrame, cosine_similarities: bool):\n",
    "    \n",
    "    # taking the title and rating to store in new data frame called animes\n",
    "    animes = df[['name', 'rating']]\n",
    "\n",
    "    #Reverse mapping of the index\n",
    "    indices = pd.Series(df.index, index = df['name']).drop_duplicates()# Recommending the Top 5 similar animes\n",
    "    # drop all duplicate occurrences of the labels \n",
    "    indices = indices.groupby(indices.index).first()\n",
    "\n",
    "    idx = indices[title]\n",
    "    sim_scores = sorted(list(enumerate(cosine_similarities[idx])), key = lambda x: x[1], reverse = True)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "    anime_indices = [i[0] for i in sim_scores]\n",
    "    recommend = animes.iloc[anime_indices]\n",
    "    \n",
    "    count = 0\n",
    "    for index, row in recommend.iterrows():\n",
    "        print('{}. {}, similarity: {}, rating: {}'.format(count+1, row['name'], sim_scores[count][1], row['rating']))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ckkok\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('man', 0.7664012312889099),\n",
       " ('girl', 0.7494640946388245),\n",
       " ('teenager', 0.631708562374115),\n",
       " ('lady', 0.6288785934448242),\n",
       " ('mother', 0.607630729675293),\n",
       " ('boy', 0.5975908041000366),\n",
       " ('she', 0.5641393661499023),\n",
       " ('person', 0.5470173358917236),\n",
       " ('housewife', 0.5463822484016418),\n",
       " ('victim', 0.545007586479187)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_model.most_similar(positive=['woman'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'ranma ½ super'\n",
    "\n",
    "# taking the title and rating to store in new data frame called animes\n",
    "animes = df[['name', 'rating']]\n",
    "\n",
    "#Reverse mapping of the index\n",
    "indices = pd.Series(df.index, index = df['name']).drop_duplicates()# Recommending the Top 5 similar animes\n",
    "# drop all duplicate occurrences of the labels \n",
    "indices = indices.groupby(indices.index).first()\n",
    "\n",
    "idx = indices[title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ckkok\\Anaconda3\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('really', 0.6926707029342651),\n",
       " ('just', 0.6821541786193848),\n",
       " ('anyway', 0.6122572422027588),\n",
       " ('defintiely', 0.5991133451461792),\n",
       " ('actually', 0.5837223529815674),\n",
       " ('certainly', 0.5828970670700073),\n",
       " ('nice', 0.5794240236282349),\n",
       " ('probably', 0.5778967142105103),\n",
       " ('so', 0.5721061825752258),\n",
       " ('maybe', 0.5714657306671143)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = df['cleaned'].iloc[idx]\n",
    "current_reviews = []\n",
    "for word in line.split():\n",
    "    if word in google_model.wv.vocab and word not in current_reviews:\n",
    "        current_reviews.append(word)\n",
    "google_model.most_similar(positive=current_reviews, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cowboy bebop</td>\n",
       "      <td>8.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uchuu kaizoku captain herlock</td>\n",
       "      <td>7.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dragon ball super saiya jin zetsumetsu keikaku</td>\n",
       "      <td>6.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>top wo nerae 2 diebuster</td>\n",
       "      <td>7.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toriko</td>\n",
       "      <td>7.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nurarihyon no mago sennen makyou</td>\n",
       "      <td>8.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>star ocean ex</td>\n",
       "      <td>6.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tenchi muyou in love</td>\n",
       "      <td>7.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>naruto x ut</td>\n",
       "      <td>7.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kämpfer für die liebe</td>\n",
       "      <td>6.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fate zero</td>\n",
       "      <td>8.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>break blade 6 doukoku no toride</td>\n",
       "      <td>7.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ranma ½ chou musabetsu kessen ranma team vs de...</td>\n",
       "      <td>7.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ranma ½ super</td>\n",
       "      <td>7.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mahou shoujo lyrical nanoha the movie 2nd as</td>\n",
       "      <td>8.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dog days</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sacred seven</td>\n",
       "      <td>6.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>no6</td>\n",
       "      <td>7.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>c the money of soul and possibility control</td>\n",
       "      <td>7.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>majutsushi orphen</td>\n",
       "      <td>7.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  rating\n",
       "0                                        cowboy bebop    8.79\n",
       "1                       uchuu kaizoku captain herlock    7.72\n",
       "2      dragon ball super saiya jin zetsumetsu keikaku    6.73\n",
       "3                            top wo nerae 2 diebuster    7.69\n",
       "4                                              toriko    7.59\n",
       "5                    nurarihyon no mago sennen makyou    8.02\n",
       "6                                       star ocean ex    6.62\n",
       "7                                tenchi muyou in love    7.44\n",
       "8                                         naruto x ut    7.42\n",
       "9                               kämpfer für die liebe    6.52\n",
       "10                                          fate zero    8.37\n",
       "11                    break blade 6 doukoku no toride    7.72\n",
       "12  ranma ½ chou musabetsu kessen ranma team vs de...    7.39\n",
       "13                                      ranma ½ super    7.67\n",
       "14       mahou shoujo lyrical nanoha the movie 2nd as    8.19\n",
       "15                                           dog days    7.01\n",
       "16                                       sacred seven    6.65\n",
       "17                                                no6    7.59\n",
       "18        c the money of soul and possibility control    7.27\n",
       "19                                  majutsushi orphen    7.19"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animes.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it's majoritily run by words that occur often. That's a good reason to begin using TFIDF (althought it takes a while to train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
