{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\olive\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\olive\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "import yake\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(use_saved_file: bool = False):\n",
    "    if (use_saved_file): \n",
    "        df = pd.read_pickle(\"./synopsys_df.pkl\") \n",
    "        return df\n",
    "\n",
    "    # Reading the data\n",
    "    df = pd.read_csv(\"../../data/anime_data.csv\")\n",
    "   \n",
    "    # Match the name and rating of animes in code_df to the anime reviews dataframe\n",
    "    df['code'] = df['code'].astype(int)    \n",
    "    df['synopsis'] = df['synopsis'].astype(str)\n",
    "    \n",
    "    df = df[[\"code\", \"name\", \"synopsis\", \"rating\"]] #keep only the relevant columns to save space\n",
    "    \n",
    "    #Utitlity functions for removing synopsis that are two short, Non-ASCII characters, converting lower case, removing stop words, html and punctuation from description\n",
    "    def removeShortSynopsis(text):\n",
    "        if len(text.split(\". \")) < 2:            \n",
    "            return None\n",
    "        else:\n",
    "            return text\n",
    "    def remove_line_ending(text):\n",
    "        return re.sub(r\" \\[Written by MAL Rewrite\\]\", \"\", text)\n",
    "        \n",
    "    def _removeNonAscii(s):\n",
    "        return \"\".join(i for i in s if ord(i)<128)\n",
    "\n",
    "    def make_lower_case(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "#     # a list of stops words that I found after some analysis\n",
    "#     f = open(\"./custom_stop_words.txt\", \"r\", encoding=\"utf-8\")\n",
    "#     custom_stops = set(map(lambda x: x[:-2], f.readlines()))\n",
    "#     stops.union(custom_stops)\n",
    "    \n",
    "    def remove_stop_words(text):\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "\n",
    "    def remove_html(text):\n",
    "        html_pattern = re.compile('<.*?>')\n",
    "        return html_pattern.sub(r'', text)\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        text = tokenizer.tokenize(text)\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "\n",
    "    df['cleaned'] = df['synopsis'].apply(_removeNonAscii)\n",
    "    df['cleaned'] = df.cleaned.apply(func = remove_line_ending)\n",
    "    df['cleaned'] = df.cleaned.apply(func = make_lower_case)\n",
    "    df['cleaned'] = df.cleaned.apply(func = remove_stop_words)\n",
    "    df['cleaned'] = df.cleaned.apply(func=remove_punctuation)\n",
    "    df['cleaned'] = df.cleaned.apply(func=remove_html)\n",
    "    df.name = df.name.apply(lambda x: re.sub(r\"\\s\\s*\", \" \", re.sub(r\"[\\-\\_]\", \" \", x)))    \n",
    "    with open(\"./synopsys_df.pkl\", \"wb\") as filehandle:\n",
    "        pickle.dump(df, filehandle)\n",
    "        filehandle.close()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(df: pd.DataFrame, use_saved_file: bool=False):\n",
    "    if (use_saved_file): \n",
    "        with open('w2v_cosine_sim.data', 'rb') as filehandle:\n",
    "            # read the data as binary data stream\n",
    "            cosine_similarities = pickle.load(filehandle)\n",
    "            return cosine_similarities\n",
    "    \n",
    "    #splitting the description into words\n",
    "    corpus = []\n",
    "    for words in df['cleaned']:\n",
    "        corpus.append(words.split())\n",
    "\n",
    "    # Using the Google pretrained Word2Vec Model \n",
    "    # If using for the first time, download and store in ../../data/ \n",
    "    # (link: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz)\n",
    "\n",
    "    EMBEDDING_FILE = '../../data/GoogleNews-vectors-negative300.bin.gz'\n",
    "    google_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "\n",
    "    # Training our corpus with Google Pretrained Model\n",
    "\n",
    "    google_model = Word2Vec(size = 300, window=5, min_count = 2, workers = -1)\n",
    "    google_model.build_vocab(corpus)\n",
    "\n",
    "    #model.intersect_word2vec_format('./word2vec/GoogleNews-vectors-negative300.bin', lockf=1.0, binary=True)\n",
    "\n",
    "    google_model.intersect_word2vec_format(EMBEDDING_FILE, lockf=1.0, binary=True)\n",
    "\n",
    "    google_model.train(corpus, total_examples=google_model.corpus_count, epochs = 5)\n",
    "\n",
    "    # Generate the average word2vec for the each set of anime reviews\n",
    "    def vectors(x: pd.DataFrame):\n",
    "        \n",
    "        # Creating a list for storing the vectors (description into vectors)\n",
    "        global array_embeddings\n",
    "        array_embeddings = []\n",
    "        kw_extractor = yake.KeywordExtractor(lan=\"en\", n=2, dedupLim=0.9, dedupFunc=\"seqm\", windowsSize=1, top=20)\n",
    "        \n",
    "        # Reading the each anime review set\n",
    "        for line in df['cleaned']:\n",
    "            avgword2vec = None\n",
    "            count = 0\n",
    "            #extract keywords from the text\n",
    "            keywords = kw_extractor.extract_keywords(line)\n",
    "            words = []\n",
    "            for word, _ in keywords:\n",
    "                words.extend(word.split())\n",
    "            words = list(set(words))\n",
    "            \n",
    "            for word in words:\n",
    "                if word in google_model.wv.vocab:\n",
    "                    count += 1\n",
    "                    if avgword2vec is None:\n",
    "                        avgword2vec = google_model[word]\n",
    "                    else:\n",
    "                        avgword2vec = avgword2vec + google_model[word]\n",
    "                    \n",
    "            if avgword2vec is not None:\n",
    "                avgword2vec = avgword2vec / count\n",
    "            \n",
    "                array_embeddings.append(avgword2vec)\n",
    "\n",
    "    # Calling the function vectors\n",
    "    vectors(df)\n",
    "\n",
    "    # finding cosine similarity for the vectors\n",
    "    cosine_similarities = cosine_similarity(array_embeddings, array_embeddings)\n",
    "\n",
    "    with open('w2v_cosine_sim.data', 'wb') as filehandle:\n",
    "        # store the data as binary data stream\n",
    "        pickle.dump(cosine_similarities, filehandle)\n",
    "\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(title: str, df: pd.DataFrame, cosine_similarities: bool):\n",
    "    \n",
    "    # taking the title and rating to store in new data frame called animes\n",
    "    animes = df[['name', 'rating']]\n",
    "\n",
    "    #Reverse mapping of the index\n",
    "    indices = pd.Series(df.index, index = df['name']).drop_duplicates()# Recommending the Top 5 similar animes\n",
    "    # drop all duplicate occurrences of the labels \n",
    "    indices = indices.groupby(indices.index).first()\n",
    "\n",
    "    idx = indices[title]\n",
    "    sim_scores = sorted(list(enumerate(cosine_similarities[idx])), key = lambda x: x[1], reverse = True)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "    anime_indices = [i[0] for i in sim_scores]\n",
    "    recommend = animes.iloc[anime_indices]\n",
    "    \n",
    "    count = 0\n",
    "    for index, row in recommend.iterrows():\n",
    "        print('{}. {}, similarity: {}, rating: {}'.format(count+1, row['name'], sim_scores[count][1], row['rating']))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = preprocess_data(use_saved_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "C:\\Users\\olive\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\olive\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:56: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# if you need to retrain or don't have the saved .data file, set use_saved_file to False\n",
    "cosine_similarities = train_word2vec(df, use_saved_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. danball senki wars, similarity: 0.7767904996871948, rating: G - All Ages\n",
      "2. no game no life, similarity: 0.776049792766571, rating: PG-13 - Teens 13 or older\n",
      "3. arslan senki tv tsuioku no shou dakkan no yaiba, similarity: 0.7740839719772339, rating: R - 17+ (violence & profanity)\n",
      "4. mahou no star magical emi, similarity: 0.7735641002655029, rating: G - All Ages\n",
      "5. kekkai sensen beyond zapp renfro ingaouhouchuu baccardio no shizuku, similarity: 0.7727901935577393, rating: R - 17+ (violence & profanity)\n"
     ]
    }
   ],
   "source": [
    "recommendations('tengen toppa gurren lagann', df, cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>name</th>\n",
       "      <th>synopsis</th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>25099</td>\n",
       "      <td>ore ga ojousama gakkou ni shomin sample toshit...</td>\n",
       "      <td>Kimito Kagurazaka is a commoner with a fetish ...</td>\n",
       "      <td>PG-13 - Teens 13 or older</td>\n",
       "      <td>kimito kagurazaka commoner fetish men s muscle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4458</th>\n",
       "      <td>31797</td>\n",
       "      <td>ore ga ojousama gakkou ni shomin sample toshit...</td>\n",
       "      <td>Kujou-san no Do-S Soudanshitsu Anime-ban short...</td>\n",
       "      <td>PG-13 - Teens 13 or older</td>\n",
       "      <td>kujou san do s soudanshitsu anime ban short an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       code                                               name  \\\n",
       "3948  25099  ore ga ojousama gakkou ni shomin sample toshit...   \n",
       "4458  31797  ore ga ojousama gakkou ni shomin sample toshit...   \n",
       "\n",
       "                                               synopsis  \\\n",
       "3948  Kimito Kagurazaka is a commoner with a fetish ...   \n",
       "4458  Kujou-san no Do-S Soudanshitsu Anime-ban short...   \n",
       "\n",
       "                         rating  \\\n",
       "3948  PG-13 - Teens 13 or older   \n",
       "4458  PG-13 - Teens 13 or older   \n",
       "\n",
       "                                                cleaned  \n",
       "3948  kimito kagurazaka commoner fetish men s muscle...  \n",
       "4458  kujou san do s soudanshitsu anime ban short an...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.name.str.contains(\"ore ga ojousama\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_extractor = yake.KeywordExtractor(lan=\"en\", n=2, dedupLim=0.9, dedupFunc=\"seqm\", windowsSize=1, top=20)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = kw_extractor.extract_keywords(df.iloc[3948].synopsis)\n",
    "words = []\n",
    "for word, _ in keywords:\n",
    "    words.extend(word.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'academy',\n",
       " 'all-girls',\n",
       " 'assumptions',\n",
       " 'club',\n",
       " 'commoner',\n",
       " 'elite',\n",
       " 'girls',\n",
       " 'hakua',\n",
       " 'jinryou',\n",
       " 'kagurazaka',\n",
       " 'karen',\n",
       " 'kimito',\n",
       " 'life',\n",
       " 'make',\n",
       " 'men',\n",
       " 'muscles',\n",
       " 'prefers',\n",
       " 'sample',\n",
       " 'school',\n",
       " 'seikain',\n",
       " 'shiodome'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
